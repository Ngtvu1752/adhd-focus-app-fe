import React, { useEffect, useRef, useState } from 'react';
import { FaceLandmarker, FilesetResolver } from "@mediapipe/tasks-vision";

interface FocusDetectorProps {
  isFocusMode: boolean; // Prop m·ªõi: Ch·ªâ ch·∫°y khi b·∫≠t Focus Mode
  onFocusChange: (status: 'FOCUSED' | 'DISTRACTED' | 'ABSENT', reason?: string) => void;
}

// C·∫•u tr√∫c d·ªØ li·ªáu hi·ªáu ch·ªânh
interface CalibrationData {
  baselineYaw: number;   // G√≥c quay ƒë·∫ßu t·ª± nhi√™n khi nh√¨n th·∫≥ng
  baselinePitch: number; // G√≥c c√∫i/ng·ª≠a t·ª± nhi√™n khi nh√¨n th·∫≥ng
  yawRange: number;      // Bi√™n ƒë·ªô quay ngang cho ph√©p
  pitchUpRange: number;  // Bi√™n ƒë·ªô ng·ª≠a l√™n cho ph√©p
  pitchDownRange: number;// Bi√™n ƒë·ªô c√∫i xu·ªëng cho ph√©p
}

export const FocusDetector: React.FC<FocusDetectorProps> = ({ isFocusMode, onFocusChange }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
  const requestRef = useRef<number>(0);
  const lastVideoTimeRef = useRef<number>(-1);
  const isPausedRef = useRef<boolean>(false);
  // --- STATE CHO QU√Å TR√åNH HI·ªÜU CH·ªàNH ---
  // Step: 0 (Ch∆∞a b·∫Øt ƒë·∫ßu), 1 (Nh√¨n T√¢m), 2 (Nh√¨n G√≥c Tr√°i Tr√™n), 3 (Nh√¨n G√≥c Ph·∫£i D∆∞·ªõi), 4 (Ho√†n t·∫•t)
  const [calibrationStep, setCalibrationStep] = useState<number>(0); 
  const [calibrationProgress, setCalibrationProgress] = useState(0);
  const [calibrationStepUI, setCalibrationStepUI] = useState<number>(0);
  // Ref d√πng ƒë·ªÉ logic loop ƒë·ªçc ƒë∆∞·ª£c gi√° tr·ªã m·ªõi nh·∫•t ngay l·∫≠p t·ª©c
  const calibrationStepRef = useRef<number>(0)
  const progressRef = useRef(0); // D√πng ref ƒë·∫øm progress cho ch√≠nh x√°c
  // D·ªØ li·ªáu thu th·∫≠p t·∫°m th·ªùi
  const tempCalibrationData = useRef<{yaw: number[], pitch: number[]}>({ yaw: [], pitch: [] });
  
  // D·ªØ li·ªáu chu·∫©n sau khi hi·ªáu ch·ªânh xong
  const calibrationConfig = useRef<CalibrationData>({
    baselineYaw: 0, baselinePitch: 0,
    yawRange: 30, pitchUpRange: 20, pitchDownRange: 45 // Gi√° tr·ªã m·∫∑c ƒë·ªãnh an to√†n
  });

  // Bi·∫øn ƒë·∫øm ch·ªëng nhi·ªÖu
  const distractionStreakRef = useRef<number>(0);
  const logCounterRef = useRef<number>(0);

  // --- 1. KH·ªûI T·∫†O AI (Ch·ªâ ch·∫°y 1 l·∫ßn khi mount) ---
  useEffect(() => {
    const initMediaPipe = async () => {
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );
      faceLandmarkerRef.current = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        runningMode: "VIDEO",
        numFaces: 1,
        refineLandmarks: true
      } as any);
      setIsModelLoaded(true);
    };
    initMediaPipe();

    return () => stopCamera(); // Cleanup khi unmount
  }, []);

  // --- 2. B·∫¨T/T·∫ÆT CAMERA D·ª∞A TR√äN FOCUS MODE ---
  useEffect(() => {
    if (isFocusMode && isModelLoaded) {
      // Khi v√†o Focus Mode: B·∫Øt ƒë·∫ßu camera v√† quy tr√¨nh hi·ªáu ch·ªânh
      startWebcam();
      updateStep(1); 
      setCalibrationProgress(0);
      progressRef.current = 0;
    } else {
      // Khi tho√°t Focus Mode: T·∫Øt camera
      stopCamera();
      setCalibrationStep(0);
    }
  }, [isFocusMode, isModelLoaded]);

  const updateStep = (step: number) => {
    calibrationStepRef.current = step; // Logic ƒë·ªçc c√°i n√†y
    setCalibrationStepUI(step);        // UI ƒë·ªçc c√°i n√†y
  };

  const startWebcam = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        // ƒê·∫£m b·∫£o video play
        videoRef.current.play(); 
        videoRef.current.addEventListener("loadeddata", predictWebcam);
      }
    } catch (err) {
      console.error("Webcam error:", err);
      alert("Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng c·∫•p quy·ªÅn!");
    }
  };

  const stopCamera = () => {
    cancelAnimationFrame(requestRef.current);
    if (videoRef.current && videoRef.current.srcObject) {
      (videoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
      videoRef.current.srcObject = null;
    }
  };

  // --- 3. V√íNG L·∫∂P X·ª¨ L√ù (LOOP) ---
  const predictWebcam = async () => {
    if (!faceLandmarkerRef.current || !videoRef.current) return;

    const startTimeMs = performance.now();
    if (lastVideoTimeRef.current !== videoRef.current.currentTime) {
      lastVideoTimeRef.current = videoRef.current.currentTime;
      const results = faceLandmarkerRef.current.detectForVideo(videoRef.current, startTimeMs);

      // üî• LOGIC KI·ªÇM TRA M·∫∂T
      if (results.faceLandmarks && results.faceLandmarks.length > 0) {
        const landmarks = results.faceLandmarks[0];
        
        const currentStep = calibrationStepRef.current;

        // Ch·ªâ ch·∫°y Calibration khi kh√¥ng b·ªã PAUSE
        if (currentStep > 0 && currentStep < 4 && !isPausedRef.current) {
          processCalibration(landmarks);
        } else if (currentStep === 4) {
          processAttention(landmarks);
        }
      } else {
         if (calibrationStepRef.current === 4) onFocusChange('ABSENT', 'Kh√¥ng th·∫•y khu√¥n m·∫∑t');
      }
    }
    
    if (isFocusMode) {
      requestRef.current = requestAnimationFrame(predictWebcam);
    }
  };

  // --- 4. LOGIC HI·ªÜU CH·ªàNH (CALIBRATION) ---
  const processCalibration = (landmarks: any[]) => {
    const headPose = calculateHeadPose(landmarks);
    
    tempCalibrationData.current.yaw.push(headPose.yaw);
    tempCalibrationData.current.pitch.push(headPose.pitch);

    // C·ªông progress
    progressRef.current += 2; 
    
    // C·∫≠p nh·∫≠t UI
    setCalibrationProgress(Math.min(progressRef.current, 100));
    
    // Debug log
    const currentStep = calibrationStepRef.current;
    if (progressRef.current % 20 === 0) {
        console.log(`Calibration Step ${currentStep}: Progress ${progressRef.current}%`);
    }

    // N·∫øu ƒë·∫ßy c√¢y -> K·∫øt th√∫c b∆∞·ªõc
    if (progressRef.current >= 100) {
        finishCalibrationStep();
        // ‚ùå KH√îNG reset progressRef ·ªü ƒë√¢y n·ªØa ƒë·ªÉ tr√°nh Race Condition
    }
  };

  const finishCalibrationStep = () => {
    // 1. T·∫°m d·ª´ng thu th·∫≠p ngay l·∫≠p t·ª©c
    isPausedRef.current = true;

    // 2. T√≠nh to√°n d·ªØ li·ªáu
    const dataCount = tempCalibrationData.current.yaw.length;
    
    // Safety check: N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu (tr√°nh NaN)
    if (dataCount === 0) {
        console.warn("Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu, th·ª≠ l·∫°i b∆∞·ªõc n√†y...");
        progressRef.current = 0;
        setCalibrationProgress(0);
        isPausedRef.current = false;
        return;
    }

    const avgYaw = tempCalibrationData.current.yaw.reduce((a, b) => a + b, 0) / dataCount;
    const avgPitch = tempCalibrationData.current.pitch.reduce((a, b) => a + b, 0) / dataCount;
    
    const currentStep = calibrationStepRef.current;
    console.log(`‚úÖ Step ${currentStep} Done. AvgYaw: ${avgYaw.toFixed(2)}, AvgPitch: ${avgPitch.toFixed(2)}`);

    // 3. L∆∞u c·∫•u h√¨nh
    if (currentStep === 1) {
      calibrationConfig.current.baselineYaw = avgYaw;
      calibrationConfig.current.baselinePitch = avgPitch;
      
      // Chuy·ªÉn sang Step 2 tr√™n UI ngay ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt m√† li·∫øc m·∫Øt
      updateStep(2);
    } 
    else if (currentStep === 2) {
      calibrationConfig.current.yawRange = Math.abs(avgYaw - calibrationConfig.current.baselineYaw) * 1.5;
      calibrationConfig.current.pitchUpRange = Math.abs(avgPitch - calibrationConfig.current.baselinePitch) * 1.5;
      
      // Chuy·ªÉn sang Step 3 tr√™n UI
      updateStep(3);
    }
    else if (currentStep === 3) {
      const currentYawRange = Math.abs(avgYaw - calibrationConfig.current.baselineYaw) * 1.5;
      const currentPitchDownRange = Math.abs(avgPitch - calibrationConfig.current.baselinePitch) * 1.5;
      
      calibrationConfig.current.yawRange = Math.max(calibrationConfig.current.yawRange, currentYawRange, 25);
      calibrationConfig.current.pitchDownRange = Math.max(currentPitchDownRange, 30);

      console.log("üéâ CALIBRATION COMPLETE:", calibrationConfig.current);
      updateStep(4); // Ho√†n t·∫•t
    }

    // 4. RESET D·ªÆ LI·ªÜU & PROGRESS
    tempCalibrationData.current = { yaw: [], pitch: [] };
    progressRef.current = 0;
    setCalibrationProgress(0);

    // 5. SET TIMEOUT ƒê·ªÇ TI·∫æP T·ª§C (T·∫°o kho·∫£ng ngh·ªâ 1.5 gi√¢y)
    // Trong 1.5s n√†y: Ch·∫•m ƒë·ªè ƒë√£ chuy·ªÉn v·ªã tr√≠ m·ªõi, nh∆∞ng progress bar ch∆∞a ch·∫°y.
    // Ng∆∞·ªùi d√πng c√≥ th·ªùi gian ƒë·ªÉ ·ªïn ƒë·ªãnh m·∫Øt t·∫°i v·ªã tr√≠ m·ªõi.
    setTimeout(() => {
        if (calibrationStepRef.current < 4) {
            console.log("‚ñ∂Ô∏è Ti·∫øp t·ª•c thu th·∫≠p d·ªØ li·ªáu...");
            isPausedRef.current = false; // M·ªü kh√≥a cho ph√©p thu th·∫≠p ti·∫øp
        } else {
             isPausedRef.current = false; // M·ªü kh√≥a cho mode gi√°m s√°t
        }
    }, 1500); // Delay 1.5 gi√¢y
  };

  // --- 5. LOGIC PH√ÅT HI·ªÜN T·∫¨P TRUNG (D√ôNG D·ªÆ LI·ªÜU ƒê√É HI·ªÜU CH·ªàNH) ---
  const processAttention = (landmarks: any[]) => {
    const headPose = calculateHeadPose(landmarks);
    const gaze = calculateGaze(landmarks);
    const config = calibrationConfig.current;

    // T√≠nh ƒë·ªô l·ªách so v·ªõi Baseline c·ªßa ng∆∞·ªùi d√πng (ch·ª© kh√¥ng ph·∫£i so v·ªõi 0)
    const relativeYaw = headPose.yaw - config.baselineYaw;
    const relativePitch = headPose.pitch - config.baselinePitch;

    let isDistracted = false;
    let reason = '';

    // So s√°nh v·ªõi Range c√° nh√¢n h√≥a
    if (Math.abs(relativeYaw) > config.yawRange) {
      isDistracted = true;
      reason = 'Quay ƒë·∫ßu qu√° nhi·ªÅu';
    } 
    // Pitch: C√∫i qu√° ng∆∞·ª°ng cho ph√©p HO·∫∂C Ng·ª≠a qu√° ng∆∞·ª°ng cho ph√©p
    // L∆∞u √Ω: Gi·∫£ ƒë·ªãnh Pitch > 0 l√† c√∫i, Pitch < 0 l√† ng·ª≠a (c·∫ßn check log th·ª±c t·∫ø)
    else if (relativePitch > config.pitchDownRange) {
      isDistracted = true;
      reason = 'C√∫i ƒë·∫ßu qu√° th·∫•p';
    }
    else if (relativePitch < -config.pitchUpRange) {
      isDistracted = true;
      reason = 'Ng·ª≠a ƒë·∫ßu qu√° cao';
    }
    // Gaze: V·∫´n d√πng ng∆∞·ª°ng c·ª©ng cho m·∫Øt v√¨ m·∫Øt di chuy·ªÉn r·∫•t nhanh
    else if (Math.abs(gaze.x) > 0.25) { // TƒÉng nh·∫π l√™n 0.25 cho ƒë·ª° nh·∫°y
      isDistracted = true;
      reason = 'M·∫Øt kh√¥ng nh√¨n m√†n h√¨nh';
    }

    // Debounce Logic (Ch·ªëng nhi·ªÖu)
    if (isDistracted) {
      distractionStreakRef.current++;
    } else {
      distractionStreakRef.current = 0;
      onFocusChange('FOCUSED');
    }

    if (distractionStreakRef.current > 90) { // ~1.5s
      onFocusChange('DISTRACTED', reason);
    }
    
    // Log throttle (ƒë·ªÉ debug)
    logCounterRef.current++;
    if(logCounterRef.current % 60 === 0) {
        console.log(`Delta Yaw: ${relativeYaw.toFixed(1)} (Limit: ${config.yawRange.toFixed(1)})`);
    }
  };

  // C√°c h√†m t√≠nh to√°n h√¨nh h·ªçc (Gi·ªØ nguy√™n nh∆∞ phi√™n b·∫£n tr∆∞·ªõc)
  const calculateHeadPose = (landmarks: any[]) => {
      const nose = landmarks[1];
      const leftEar = landmarks[454];
      const rightEar = landmarks[234];
      const chin = landmarks[152];
      const forehead = landmarks[10];
  
      // Yaw
      const distToLeft = Math.abs(nose.x - leftEar.x);
      const distToRight = Math.abs(nose.x - rightEar.x);
      const yawRatio = (distToLeft - distToRight) / (distToLeft + distToRight);
      const yaw = yawRatio * 90; 
  
      // Pitch (Gi·∫£ ƒë·ªãnh: D∆∞∆°ng l√† c√∫i, √Çm l√† ng·ª≠a)
      const faceHeight = Math.abs(chin.y - forehead.y);
      const noseY = nose.y;
      const midY = (chin.y + forehead.y) / 2;
      const pitchRatio = (noseY - midY) / faceHeight; 
      const pitch = pitchRatio * 180; 
  
      return { yaw, pitch };
  };

  const calculateGaze = (landmarks: any[]) => {
      const leftIris = landmarks[468];
      const rightIris = landmarks[473];
      const getEyeRatio = (iris: any, inner: any, outer: any) => {
          const width = Math.abs(outer.x - inner.x);
          const dist = Math.abs(iris.x - inner.x);
          return (dist / width) - 0.5; 
      };
      const leftRatio = getEyeRatio(leftIris, landmarks[33], landmarks[133]);
      const rightRatio = getEyeRatio(rightIris, landmarks[362], landmarks[263]);
      return { x: (leftRatio + rightRatio) / 2 };
  };

  // --- RENDER GIAO DI·ªÜN ---
  // N·∫øu kh√¥ng ·ªü Focus Mode -> Kh√¥ng render g√¨ c·∫£ (ho·∫∑c null)
  if (!isFocusMode) {
    return null;
    }
  return (
    <>
      <video 
        ref={videoRef} 
        autoPlay 
        playsInline 
        muted 
        className="hidden" 
        />

      {/* S·ª≠ d·ª•ng calibrationStepUI (State) ƒë·ªÉ render */}
      {calibrationStepUI > 0 && calibrationStepUI < 4 && (
        <div className="fixed inset-0 z-[9999] bg-black/80 flex flex-col items-center justify-center text-white">
          <h2 className="text-2xl font-bold mb-4">‚öôÔ∏è Thi·∫øt l·∫≠p g√≥c nh√¨n</h2>
          <p className="mb-8 text-gray-300">Gi·ªØ nguy√™n ƒë·∫ßu v√† nh√¨n theo ch·∫•m ƒë·ªè nh√©!</p>
          
          <div className="w-64 h-2 bg-gray-700 rounded-full mb-8">
            <div 
              className="h-full bg-[#FFD966] rounded-full transition-all duration-100" 
              style={{ width: `${calibrationProgress}%` }}
            />
          </div>

          <div 
            className="absolute w-8 h-8 bg-red-500 rounded-full shadow-[0_0_20px_rgba(255,0,0,0.8)] animate-pulse transition-all duration-500"
            style={{
              // Logic v·ªã tr√≠ ch·∫•m ƒë·ªè
              top: calibrationStepUI === 1 ? '50%' : (calibrationStepUI === 2 ? '20%' : '80%'),
              left: calibrationStepUI === 1 ? '50%' : (calibrationStepUI === 2 ? '20%' : '80%'),
              transform: 'translate(-50%, -50%)'
            }}
          />

          <p className="text-xl font-bold mt-10">
            {calibrationStepUI === 1 && "Nh√¨n th·∫≥ng v√†o gi·ªØa m√†n h√¨nh"}
            {calibrationStepUI === 2 && "Nh√¨n l√™n g√≥c TR√ÅI m√†n h√¨nh"}
            {calibrationStepUI === 3 && "Nh√¨n xu·ªëng g√≥c PH·∫¢I m√†n h√¨nh"}
          </p>
        </div>
      )}
    </>
  );
};