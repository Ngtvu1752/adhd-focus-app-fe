import React, { useEffect, useRef, useState } from 'react';
import { FaceLandmarker, FilesetResolver } from "@mediapipe/tasks-vision";

export type MascotMood = 'happy' | 'focused' | 'celebrating' | 'resting' | 'frustrated' | 'bored' | 'stressed' | 'surprised';

interface FocusDetectorProps {
  isFocusMode: boolean;
  onFocusChange: (status: 'FOCUSED' | 'DISTRACTED' | 'ABSENT', reason?: string) => void;
  onMoodChange?: (mood: MascotMood) => void;
}
// Cáº¥u trÃºc dá»¯ liá»‡u hiá»‡u chá»‰nh
interface CalibrationData {
  baselineYaw: number;   // GÃ³c quay Ä‘áº§u tá»± nhiÃªn khi nhÃ¬n tháº³ng
  baselinePitch: number; // GÃ³c cÃºi/ngá»­a tá»± nhiÃªn khi nhÃ¬n tháº³ng
  yawRange: number;      // BiÃªn Ä‘á»™ quay ngang cho phÃ©p
  pitchUpRange: number;  // BiÃªn Ä‘á»™ ngá»­a lÃªn cho phÃ©p
  pitchDownRange: number;// BiÃªn Ä‘á»™ cÃºi xuá»‘ng cho phÃ©p
}

export const FocusDetector: React.FC<FocusDetectorProps> = ({ isFocusMode, onFocusChange, onMoodChange }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
  const requestRef = useRef<number>(0);
  const lastVideoTimeRef = useRef<number>(-1);
  const isPausedRef = useRef<boolean>(false);
  const lastValidHeadPoseRef = useRef<{ yaw: number, pitch: number }>({ yaw: 0, pitch: 0 });
  const missingFaceFramesRef = useRef<number>(0);
  const lastMoodRef = useRef<MascotMood>('focused');
  const smoothedScoresRef = useRef({
    smile: 0,
    frown: 0,
    surprise: 0
  });
  // Step: 0 (ChÆ°a báº¯t Ä‘áº§u), 1 (NhÃ¬n TÃ¢m), 2 (NhÃ¬n GÃ³c TrÃ¡i TrÃªn), 3 (NhÃ¬n GÃ³c Pháº£i DÆ°á»›i), 4 (HoÃ n táº¥t)
  const [calibrationStep, setCalibrationStep] = useState<number>(0); 
  const [calibrationProgress, setCalibrationProgress] = useState(0);
  const [calibrationStepUI, setCalibrationStepUI] = useState<number>(0);
  // Ref dÃ¹ng Ä‘á»ƒ logic loop Ä‘á»c Ä‘Æ°á»£c giÃ¡ trá»‹ má»›i nháº¥t ngay láº­p tá»©c
  const calibrationStepRef = useRef<number>(0)
  const progressRef = useRef(0); // DÃ¹ng ref Ä‘áº¿m progress cho chÃ­nh xÃ¡c
  // Dá»¯ liá»‡u thu tháº­p táº¡m thá»i
  const tempCalibrationData = useRef<{yaw: number[], pitch: number[]}>({ yaw: [], pitch: [] });
  
  // Dá»¯ liá»‡u chuáº©n sau khi hiá»‡u chá»‰nh xong
  const calibrationConfig = useRef<CalibrationData>({
    baselineYaw: 0, baselinePitch: 0,
    yawRange: 30, pitchUpRange: 20, pitchDownRange: 45 // GiÃ¡ trá»‹ máº·c Ä‘á»‹nh an toÃ n
  });

  // Biáº¿n Ä‘áº¿m chá»‘ng nhiá»…u
  const distractionStreakRef = useRef<number>(0);
  const logCounterRef = useRef<number>(0);

  useEffect(() => {
    const initMediaPipe = async () => {
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );
      faceLandmarkerRef.current = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        runningMode: "VIDEO",
        numFaces: 1,
        refineLandmarks: true,
        minFaceDetectionConfidence: 0.2, 
        minFacePresenceConfidence: 0.2
      } as any);
      setIsModelLoaded(true);
    };
    initMediaPipe();

    return () => stopCamera(); // Cleanup khi unmount
  }, []);

  useEffect(() => {
    if (isFocusMode && isModelLoaded) {
      startWebcam();
      updateStep(1); 
      setCalibrationProgress(0);
      progressRef.current = 0;
    } else {
      stopCamera();
      setCalibrationStep(0);
    }
  }, [isFocusMode, isModelLoaded]);

  const updateStep = (step: number) => {
    calibrationStepRef.current = step; 
    setCalibrationStepUI(step);        
  };

  const startWebcam = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        // Äáº£m báº£o video play
        videoRef.current.play(); 
        videoRef.current.addEventListener("loadeddata", predictWebcam);
      }
    } catch (err) {
      console.error("Webcam error:", err);
      alert("KhÃ´ng thá»ƒ má»Ÿ camera. Vui lÃ²ng cáº¥p quyá»n!");
    }
  };

  const stopCamera = () => {
    if (requestRef.current) {
      cancelAnimationFrame(requestRef.current);
      requestRef.current = 0;
    }

    if (videoRef.current && videoRef.current.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      const tracks = stream.getTracks();
      
      tracks.forEach(track => {
        track.stop(); // Lá»‡nh nÃ y sáº½ táº¯t Ä‘Ã¨n xanh trÃªn camera váº­t lÃ½
        // console.log("ğŸ“· Camera Track Stopped:", track.label);
      });

      videoRef.current.srcObject = null;
    }
    
  };

  const predictWebcam = async () => {
    if (!faceLandmarkerRef.current || !videoRef.current) return;

    const startTimeMs = performance.now();
    if (lastVideoTimeRef.current !== videoRef.current.currentTime) {
      lastVideoTimeRef.current = videoRef.current.currentTime;
      const results = faceLandmarkerRef.current.detectForVideo(videoRef.current, startTimeMs);

      const currentStep = calibrationStepRef.current;

      if (results.faceLandmarks && results.faceLandmarks.length > 0) {
        missingFaceFramesRef.current = 0; // Reset bá»™ Ä‘áº¿m

        const landmarks = results.faceLandmarks[0];
        
        // Cáº­p nháº­t tÆ° tháº¿ cuá»‘i cÃ¹ng
        const currentPose = calculateHeadPose(landmarks);
        lastValidHeadPoseRef.current = currentPose;

        if (currentStep > 0 && currentStep < 4 && !isPausedRef.current) {
          processCalibration(landmarks);
        } else if (currentStep === 4) {
          processAttention(landmarks);
        }

        if (results.faceBlendshapes && results.faceBlendshapes.length > 0) {
           detectEmotion(results.faceBlendshapes[0].categories);
        }
      } else {
         // Máº¤T Dáº¤U KHUÃ”N Máº¶T
        
         // TÄƒng bá»™ Ä‘áº¿m frame bá»‹ máº¥t
         missingFaceFramesRef.current++;

         // Láº¤Y Dá»® LIá»†U CUá»I CÃ™NG Äá»‚ SUY LUáº¬N
         const lastPitch = lastValidHeadPoseRef.current.pitch - calibrationConfig.current.baselinePitch;
         const lastYaw = lastValidHeadPoseRef.current.yaw - calibrationConfig.current.baselineYaw;

         // Äiá»u kiá»‡n: GÃ³c cÃºi cuá»‘i cÃ¹ng > 15 Ä‘á»™ (tÆ°Æ¡ng Ä‘á»‘i so vá»›i baseline)
         const isLookingDown = lastPitch > 15; 
         
         // Náº¿u Ä‘ang cÃºi viáº¿t bÃ i, cho phÃ©p máº¥t máº·t tá»›i 450 frames (khoáº£ng 15 giÃ¢y)
         // Náº¿u chá»‰ quay Ä‘áº§u, chá»‰ cho phÃ©p 90 frames (3 giÃ¢y)
         const limitFrames = isLookingDown ? 450 : 90;

         if (missingFaceFramesRef.current < limitFrames) {
             
             if (isLookingDown) {
                 // Náº¿u Ä‘ang cÃºi viáº¿t bÃ i -> Coi lÃ  FOCUSED (Táº­p trung)
                 onFocusChange('FOCUSED'); 
                 
                 // Giá»¯ UI mÃ u xanh (Fake detected) Ä‘á»ƒ bÃ© khÃ´ng bá»‹ xao nhÃ£ng
             } else {
                 onFocusChange('DISTRACTED', 'MÃ¬nh cÃ¹ng nhÃ¬n tháº³ng vÃ o mÃ n hÃ¬nh nhÃ©!');
             }
         } else {
             onFocusChange('ABSENT', 'Báº¡n Æ¡i, quay láº¡i gháº¿ ngá»“i nÃ o!');
         }
      }
    }
    
    if (isFocusMode) {
      requestRef.current = requestAnimationFrame(predictWebcam);
    }
  };

  const detectEmotion = (blendshapes: any[]) => {
    const getScore = (name: string) => blendshapes.find(b => b.categoryName === name)?.score || 0;

    // 1. Láº¥y dá»¯ liá»‡u thÃ´ (Raw Data)
    const rawSmile = (getScore('mouthSmileLeft') + getScore('mouthSmileRight')) / 2;
    const rawFrown = (getScore('browDownLeft') + getScore('browDownRight')) / 2;
    const rawSurprise = getScore('browInnerUp');

    const smoothFactor = 0.7; 
    
    smoothedScoresRef.current.smile = (smoothedScoresRef.current.smile * smoothFactor) + (rawSmile * (1 - smoothFactor));
    smoothedScoresRef.current.frown = (smoothedScoresRef.current.frown * smoothFactor) + (rawFrown * (1 - smoothFactor));
    smoothedScoresRef.current.surprise = (smoothedScoresRef.current.surprise * smoothFactor) + (rawSurprise * (1 - smoothFactor));

    const { smile, frown, surprise } = smoothedScoresRef.current;

    const currentMood = lastMoodRef.current;

    let detectedMood: MascotMood = 'focused'; // Máº·c Ä‘á»‹nh

    const isSmiling = currentMood === 'happy' || currentMood === 'celebrating'
        ? smile > 0.4 
        : smile > 0.6;

    // --- LOGIC STRESSED ---
    const isStressed = currentMood === 'stressed' 
        ? frown > 0.4 
        : frown > 0.55;

    // --- LOGIC SURPRISED ---
    const isSurprised = currentMood === 'surprised' 
        ? surprise > 0.4 
        : surprise > 0.55;

    // 4. Quyáº¿t Ä‘á»‹nh Mood (Æ¯u tiÃªn)
    if (isSmiling) {
        detectedMood = smile > 0.85 ? 'celebrating' : 'happy';
    } else if (isStressed) {
        detectedMood = 'stressed';
    } else if (isSurprised) {
        detectedMood = 'surprised';
    }

    // 5. Chá»‰ update khi cÃ³ thay Ä‘á»•i
    if (currentMood !== detectedMood) {
        lastMoodRef.current = detectedMood;
        if (onMoodChange) onMoodChange(detectedMood);
        console.log(`Mood changed: ${detectedMood} (Smile: ${smile.toFixed(2)})`);
    }
  };

  const handleMoodUpdate = (newMood: MascotMood) => {
      if (lastMoodRef.current !== newMood) {
          lastMoodRef.current = newMood;
          if (onMoodChange) onMoodChange(newMood);
      }
  };
  // --- 4. LOGIC HIá»†U CHá»ˆNH (CALIBRATION) ---
  const processCalibration = (landmarks: any[]) => {
    const headPose = calculateHeadPose(landmarks);
    
    tempCalibrationData.current.yaw.push(headPose.yaw);
    tempCalibrationData.current.pitch.push(headPose.pitch);

    // Cá»™ng progress
    progressRef.current += 2; 
    
    // Cáº­p nháº­t UI
    setCalibrationProgress(Math.min(progressRef.current, 100));
    
    // Debug log
    const currentStep = calibrationStepRef.current;
    if (progressRef.current % 20 === 0) {
        console.log(`Calibration Step ${currentStep}: Progress ${progressRef.current}%`);
    }

    // Náº¿u Ä‘áº§y cÃ¢y -> Káº¿t thÃºc bÆ°á»›c
    if (progressRef.current >= 100) {
        finishCalibrationStep();
        // âŒ KHÃ”NG reset progressRef á»Ÿ Ä‘Ã¢y ná»¯a Ä‘á»ƒ trÃ¡nh Race Condition
    }
  };

  const finishCalibrationStep = () => {
    isPausedRef.current = true;

    const dataCount = tempCalibrationData.current.yaw.length;
    
    if (dataCount === 0) {
        console.warn("KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u, thá»­ láº¡i bÆ°á»›c nÃ y...");
        progressRef.current = 0;
        setCalibrationProgress(0);
        isPausedRef.current = false;
        return;
    }

    const avgYaw = tempCalibrationData.current.yaw.reduce((a, b) => a + b, 0) / dataCount;
    const avgPitch = tempCalibrationData.current.pitch.reduce((a, b) => a + b, 0) / dataCount;
    
    const currentStep = calibrationStepRef.current;
    console.log(`âœ… Step ${currentStep} Done. AvgYaw: ${avgYaw.toFixed(2)}, AvgPitch: ${avgPitch.toFixed(2)}`);

    if (currentStep === 1) {
      calibrationConfig.current.baselineYaw = avgYaw;
      calibrationConfig.current.baselinePitch = avgPitch;
      
      // Chuyá»ƒn sang Step 2 trÃªn UI ngay Ä‘á»ƒ ngÆ°á»i dÃ¹ng biáº¿t mÃ  liáº¿c máº¯t
      updateStep(2);
    } 
    else if (currentStep === 2) {
      // TÃ­nh biÃªn Ä‘á»™
      const rawYawRange = Math.abs(avgYaw - calibrationConfig.current.baselineYaw);
      const rawPitchUpRange = Math.abs(avgPitch - calibrationConfig.current.baselinePitch);
      calibrationConfig.current.yawRange = (rawYawRange * 1.2) + 10;
      calibrationConfig.current.pitchUpRange = (rawPitchUpRange * 1.2) + 5;
      
      updateStep(3);
    }
    else if (currentStep === 3) {
      const currentYawRange = Math.abs(avgYaw - calibrationConfig.current.baselineYaw) * 1.5;
      const currentPitchDownRange = Math.abs(avgPitch - calibrationConfig.current.baselinePitch) * 1.5;
      
      calibrationConfig.current.yawRange = Math.max(calibrationConfig.current.yawRange, currentYawRange, 25);
      calibrationConfig.current.pitchDownRange = Math.max(currentPitchDownRange, 30);

      console.log("CALIBRATION COMPLETE:", calibrationConfig.current);
      updateStep(4); // HoÃ n táº¥t
    }

    tempCalibrationData.current = { yaw: [], pitch: [] };
    progressRef.current = 0;
    setCalibrationProgress(0);

    setTimeout(() => {
        if (calibrationStepRef.current < 4) {
            console.log("â–¶ï¸ Tiáº¿p tá»¥c thu tháº­p dá»¯ liá»‡u...");
            isPausedRef.current = false; 
        } else {
             isPausedRef.current = false; 
        }
    }, 1500); // Delay 1.5 giÃ¢y
  };

  const processAttention = (landmarks: any[]) => {
    const headPose = calculateHeadPose(landmarks);
    const gaze = calculateGaze(landmarks);
    const config = calibrationConfig.current;

    const relativeYaw = headPose.yaw - config.baselineYaw;
    const relativePitch = headPose.pitch - config.baselinePitch;

    let isDistracted = false;
    let reason = '';

    // So sÃ¡nh vá»›i Range cÃ¡ nhÃ¢n hÃ³a
    if (Math.abs(relativeYaw) > config.yawRange) {
      isDistracted = true;
      reason = 'MÃ¬nh cÃ¹ng nhÃ¬n tháº³ng vÃ o mÃ n hÃ¬nh nhÃ©!';
    } 
    // Pitch: CÃºi quÃ¡ ngÆ°á»¡ng cho phÃ©p HOáº¶C Ngá»­a quÃ¡ ngÆ°á»¡ng cho phÃ©p
    // LÆ°u Ã½: Giáº£ Ä‘á»‹nh Pitch > 0 lÃ  cÃºi, Pitch < 0 lÃ  ngá»­a (cáº§n check log thá»±c táº¿)
    else if (relativePitch > config.pitchDownRange) {
      isDistracted = true;
      reason = 'Ngá»“i tháº³ng lÆ°ng lÃªn cho khá»e nÃ o!';
    }
    else if (relativePitch < -config.pitchUpRange) {
      isDistracted = true;
      reason = 'NhÃ¬n xuá»‘ng bÃ i há»c chÃºt xÃ­u nÃ o!';
    }
    // Gaze: Váº«n dÃ¹ng ngÆ°á»¡ng cá»©ng cho máº¯t vÃ¬ máº¯t di chuyá»ƒn ráº¥t nhanh
    else if (Math.abs(gaze.x) > 0.25) { // TÄƒng nháº¹ lÃªn 0.25 cho Ä‘á»¡ nháº¡y
      isDistracted = true;
      reason = 'Máº¯t xinh táº­p trung vÃ o bÃ i nhÃ©!';
    }

    // Debounce Logic (Chá»‘ng nhiá»…u)
    if (isDistracted) {
      distractionStreakRef.current++;
    } else {
      distractionStreakRef.current = 0;
      onFocusChange('FOCUSED');
    }

    if (distractionStreakRef.current > 90) { // ~1.5s
      onFocusChange('DISTRACTED', reason);
    }
    
    // Log throttle (Ä‘á»ƒ debug)
    logCounterRef.current++;
    if(logCounterRef.current % 60 === 0) {
        console.log(`Delta Yaw: ${relativeYaw.toFixed(1)} (Limit: ${config.yawRange.toFixed(1)})`);
    }
  };

  const calculateHeadPose = (landmarks: any[]) => {
      const nose = landmarks[1];
      const leftEar = landmarks[454];
      const rightEar = landmarks[234];
      const chin = landmarks[152];
      const forehead = landmarks[10];
  
      // Yaw
      const distToLeft = Math.abs(nose.x - leftEar.x);
      const distToRight = Math.abs(nose.x - rightEar.x);
      const yawRatio = (distToLeft - distToRight) / (distToLeft + distToRight);
      const yaw = yawRatio * 90; 
  
      // Pitch (Giáº£ Ä‘á»‹nh: DÆ°Æ¡ng lÃ  cÃºi, Ã‚m lÃ  ngá»­a)
      const faceHeight = Math.abs(chin.y - forehead.y);
      const noseY = nose.y;
      const midY = (chin.y + forehead.y) / 2;
      const pitchRatio = (noseY - midY) / faceHeight; 
      const pitch = pitchRatio * 180; 
  
      return { yaw, pitch };
  };

  const calculateGaze = (landmarks: any[]) => {
      const leftIris = landmarks[468];
      const rightIris = landmarks[473];
      const getEyeRatio = (iris: any, inner: any, outer: any) => {
          const width = Math.abs(outer.x - inner.x);
          const dist = Math.abs(iris.x - inner.x);
          return (dist / width) - 0.5; 
      };
      const leftRatio = getEyeRatio(leftIris, landmarks[33], landmarks[133]);
      const rightRatio = getEyeRatio(rightIris, landmarks[362], landmarks[263]);
      return { x: (leftRatio + rightRatio) / 2 };
  };

  // Náº¿u khÃ´ng á»Ÿ Focus Mode -> KhÃ´ng render gÃ¬ cáº£ (hoáº·c null)
  if (!isFocusMode) {
    return null;
    }
  return (
    <>
      <video 
        ref={videoRef} 
        autoPlay 
        playsInline 
        muted 
        className="fixed bottom-4 right-4 w-32 h-24 rounded-lg border-2 border-white shadow-lg z-[50] object-cover" 
        />

      {/* Sá»­ dá»¥ng calibrationStepUI (State) Ä‘á»ƒ render */}
      {calibrationStepUI > 0 && calibrationStepUI < 4 && (
        <div className="fixed inset-0 z-[9999] bg-black/80 flex flex-col items-center justify-center text-white">
          <h2 className="text-2xl font-bold mb-4">âš™ï¸ Thiáº¿t láº­p gÃ³c nhÃ¬n</h2>
          <p className="mb-8 text-gray-300">Giá»¯ nguyÃªn Ä‘áº§u vÃ  nhÃ¬n theo cháº¥m Ä‘á» nhÃ©!</p>
          
          <div className="w-64 h-2 bg-gray-700 rounded-full mb-8">
            <div 
              className="h-full bg-[#FFD966] rounded-full transition-all duration-100" 
              style={{ width: `${calibrationProgress}%` }}
            />
          </div>

          <div 
            className="absolute w-8 h-8 bg-red-500 rounded-full shadow-[0_0_20px_rgba(255,0,0,0.8)] animate-pulse transition-all duration-500"
            style={{
              top: calibrationStepUI === 1 ? '50%' : (calibrationStepUI === 2 ? '10%' : '90%'),
              left: calibrationStepUI === 1 ? '50%' : (calibrationStepUI === 2 ? '10%' : '90%'),
              transform: 'translate(-50%, -50%)'
            }}
          />

          <p className="mb-8 text-gray-300">
            HÃ£y <span className="text-[#FFD966] font-bold">xoay Ä‘áº§u tá»± nhiÃªn</span> Ä‘á»ƒ nhÃ¬n vÃ o cháº¥m Ä‘á» nhÃ©!
          </p>
        </div>
      )}
    </>
  );
};